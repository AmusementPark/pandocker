version: '3'
services:
    spark-master:
        image: ilpan/apache-spark
        container_name: spark-master
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            # 该目录将来用
            - /data/spark/master:/data/spark
            - ../conf:/usr/local/.jenv/candidates/spark/current/conf/
        ports:
            # spark 相关端口映射
            - "8080:8080"
            - "7077:7077"
            - "4040:4040"
        hostname: spark-master
        networks:
            cluster-network:
                aliases:
                    - spark-master
        depends_on:
            - spark-worker1
            - spark-worker2
            - spark-worker3
    spark-worker1:
        image: ilpan/apache-spark
        container_name: spark-worker1
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            - /data/spark/worker1:/data/spark
            - ../conf:/usr/local/.jenv/candidates/spark/current/conf/
        ports:
            # spark 相关端口随机映射
            - "8080"
            - "7077"
            - "4040"
        hostname: spark-worker1
        networks:
            cluster-network:
                aliases:
                    - spark-worker1
    spark-worker2:
        image: ilpan/apache-spark
        container_name: spark-worker2
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            - /data/spark/worker2:/data/spark
        ports:
            - "8080"
            - "7077"
            - "4040"
        hostname: spark-worker2
        networks:
            cluster-network:
                aliases:
                    - spark-worker2
    spark-worker3:
        image: ilpan/apache-spark
        container_name: spark-worker3
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            - /data/spark/worker3:/data/spark
        ports:
            - "8080"
            - "7077"
            - "4040"
        hostname: spark-worker3
        networks:
            cluster-network:
                aliases:
                    - spark-worker3
networks:
    cluster-network:
        driver: bridge
        ipam:
            config:
                - subnet: 172.88.0.0/16
