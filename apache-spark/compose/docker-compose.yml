version: '3'
services:
    spark-master:
        image: ilpan/apache-spark
        container_name: spark-master
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            # 映射 hdfs 文件目录
            # /data/hadoop-master/data/hadoop/zookeeper:/data/hadoop/zookeeper
            - /data/hadoop-master/data:/data/
        ports:
            # hadoop 相关端口映射
            - "50070:50070"
            - "8088:8088"
            # spark 相关端口映射
            - "8080:8080"
            - "7077:7077"
            - "4040:4040"
        hostname: hadoop-master
        extra_hosts:
            - "hadoop-slave1: 172.20.0.11"
            - "hadoop-slave2: 172.20.0.12"
            - "hadoop-slave3: 172.20.0.13"
        networks:
            spark-network:
                ipv4_address: 172.20.0.10
                aliases:
                    - spark-master
        depends_on:
            - spark-worker1
            - spark-worker2
            - spark-worker3
    spark-worker1:
        image: ilpan/apache-spark
        container_name: spark-worker1
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            # 映射 hdfs 文件目录
            - /data/hadoop-slave1/data:/data/
        ports:
            # spark 相关端口随机映射
            - "8080"
            - "7077"
            - "4040"
        hostname: hadoop-slave1
        extra_hosts:
            - "hadoop-master: 172.20.0.10"
            - "hadoop-slave2: 172.20.0.12"
            - "hadoop-slave3: 172.20.0.13"
        networks:
            spark-network:
                ipv4_address: 172.20.0.11
                aliases:
                    - spark-worker1
    spark-worker2:
        image: ilpan/apache-spark
        container_name: spark-worker2
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            - /data/hadoop-slave2/data:/data/
        ports:
            - "8080"
            - "7077"
            - "4040"
        hostname: hadoop-slave2
        extra_hosts:
            - "hadoop-master: 172.20.0.10"
            - "hadoop-slave1: 172.20.0.11"
            - "hadoop-slave3: 172.20.0.13"
        networks:
            spark-network:
                ipv4_address: 172.20.0.12
                aliases:
                    - spark-worker2
    spark-worker3:
        image: ilpan/apache-spark
        container_name: spark-worker3
        command: "sudo /usr/sbin/sshd -D"
        tty: true
        stdin_open: true
        volumes:
            - /data/hadoop-slave3/data:/data
        ports:
            - "8080"
            - "7077"
            - "4040"
        hostname: hadoop-slave3
        extra_hosts:
            - "hadoop-master: 172.20.0.10"
            - "hadoop-slave1: 172.20.0.11"
            - "hadoop-slave2: 172.20.0.12"
        networks:
            spark-network:
                ipv4_address: 172.20.0.13
                aliases:
                    - spark-worker3
networks:
    spark-network:
        driver: bridge
        ipam:
            config:
                - subnet: 172.20.0.0/24
